


# Import necessary libraries
import pandas as pd


# Load the datasets using raw string notation to avoid escape character issues
data1 = pd.read_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_datan1.csv')
data2 = pd.read_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_datan2.csv')
data3 = pd.read_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_datan3.csv')


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())


# Convert the 'year' column to a datetime object if it's not already for data3
data3['year'] = pd.to_datetime(data3['year'], errors='coerce')  # Coerce invalid formats to NaT


# Extract only the year part
data3['year'] = data3['year'].dt.year.astype(str)


# Save the cleaned data3 for verification if needed
data3.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_datan3.csv', index=False)


# Preview the cleaned year column
print(data3[['title', 'year']].head())


# Transform the 'rating score' column for data2
data2['rating'] = (data2['user rating score'] / 10).round(1)


# Drop the old 'user rating score' column if no longer needed
data2 = data2.drop(columns=['user rating score'])


# Save the updated dataset for verification if needed
data2.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_datan2.csv', index=False)


# Preview the new rate column
print(data2[['title', 'rating']].head())


# Rename the column 'imdb_score' to 'rating' from data 3
data3.rename(columns={'imdb_score': 'rating'}, inplace=True)


# Save the updated dataset for verification if needed
data3.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_datan3.csv', index=False)


# Preview the new rate column
print(data3.head())


# Rename the column 'release year' to 'year' from data 2
data2.rename(columns={'release year': 'year'}, inplace=True)


# Save the updated dataset for verification if needed
data2.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_datan2.csv', index=False)


# Preview the new year column
print(data2.head())


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())


# Check for missing values
print("Data 1:")
print(data1.isnull().sum())

print("\nData 2:")
print(data2.isnull().sum())

print("\nData 3:")
print(data3.isnull().sum())


# Step 1: Merge Dataset 1 and Dataset 2 on 'title' , 'year' and 'rating'
merged_1_2 = pd.merge(data1, data2, on=['title', 'year' , 'rating'], how='outer', suffixes=('_data1', '_data2'))


# Convert 'year' column to int in both DataFrames to ensure they are of the same type
merged_1_2['year'] = merged_1_2['year'].astype(int)  # Convert to int
data3['year'] = data3['year'].astype(int)  # Convert to int


# Step 2: Merge the result with Dataset 3 on 'title' , 'year' , 'rating' and 'genre'
final_merged_data = pd.merge(merged_1_2, data3, on=['title', 'year', 'rating', 'genre'], how='outer', suffixes=('', '_data3'))


# Save the final merged dataset
output_path = r'C:\Users\saman\Downloads\FP_BA_Netflix\final_merged_data.csv'
final_merged_data.to_csv(output_path, index=False)


# Preview the new dataset
print(final_merged_data.head())


# Check for missing values
print(final_merged_data.isnull().sum())


# Remove '#' from the 'title' column in the merged dataset
final_merged_data['title'] = final_merged_data['title'].str.replace('#', '', regex=False)


# Save the updated dataset
output_path = r'C:\Users\saman\Downloads\FP_BA_Netflix\final_merged_data.csv'
final_merged_data.to_csv(output_path, index=False)


# Preview the new dataset
print(final_merged_data.head())


# Remove '#' from the 'description' column in the merged dataset
final_merged_data['description'] = final_merged_data['description'].str.replace('#', '', regex=False)


# Save the updated dataset
output_path = r'C:\Users\saman\Downloads\FP_BA_Netflix\final_merged_data.csv'
final_merged_data.to_csv(output_path, index=False)


# Preview the new dataset
print(final_merged_data.head())





import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Check the first few rows of the data
print(final_merged_data.head())


# Get a summary of the dataframe
print(final_merged_data.info())


# Describe the numerical columns
print(final_merged_data.describe())


# Check for missing values
print(final_merged_data.isnull().sum())


# Load the new dataset using raw string notation to avoid escape character issues
data4 = pd.read_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_datan4.csv')


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())

print("\nData 4:")
print(data4.head())

print("\nMerge Data:")
print(final_merged_data.head())


# Rename the column 'release_year' to 'year' from data 4
data4.rename(columns={'release_year': 'year'}, inplace=True)


# Drop the old 'id' and 'imdb_id'column if no longer needed
data4 = data4.drop(columns=['id', 'imdb_id'])


# Create the 'rating' column as the average of 'imdb_score' and 'tmdb_score', rounded to 1 decimal place
data4['rating'] = data4[['imdb_score', 'tmdb_score']].mean(axis=1).round(1)


# Drop the old 'id' and 'imdb_id'column if no longer needed
data4 = data4.drop(columns=['imdb_score', 'tmdb_score'])


# Display the updated DataFrame 4
print(data4.head())


# Remove '[' and ']' from the 'genres' column
data4['genre'] = data4['genres'].str.strip("[]").str.replace("'", "")


# Drop the old 'genres' column (optional)
data4 = data4.drop(columns=['genres'])


# Display the updated DataFrame 4
print(data4.head())


# Rename the column 'age_certification' to 'certification' from data 4
data4.rename(columns={'age_certification' : 'certification'}, inplace=True)


# Remove '[' and ']' from the 'production_countries' column
data4['production_countries'] = data4['production_countries'].str.strip("[]").str.replace("'", "")


# Display the updated DataFrame 4
print(data4.head())


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())

print("\nData 4:")
print(data4.head())


# Remove '[' and ']' from the 'stars' column
data1['stars'] = data1['stars'].str.strip("[]").str.replace("'", "")


# Rename the column 'ratingLevel' to 'warnings' from data 2
data2.rename(columns={'ratingLevel' : 'warnings'}, inplace=True)


# Drop the old 'genres' column (optional) from data 3
data3 = data3.drop(columns=['premiere'])


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())

print("\nData 4:")
print(data4.head())


# Drop the old 'tmdb_popularity' column from data 4
data4 = data4.drop(columns=['tmdb_popularity'])


# Rename the column 'imdb_votes' to 'votes' from data 4
data4.rename(columns={'imdb_votes' : 'votes'}, inplace=True)


# Drop the old 'ratingDescription' column from data 2
data2 = data2.drop(columns=['ratingDescription'])


# Drop the old 'title_age' column from data 4
data4 = data4.drop(columns=['title_age'])


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())

print("\nData 4:")
print(data4.head())


# Clean up double commas and extra spaces in the 'stars' column on data 1
data1['stars'] = data1['stars'].str.replace(', ,', ',', regex=True).str.strip(', ').str.replace(', ,', ',')


# Rename the column 'certification' to 'certificate' from data 4
data4.rename(columns={'certification' : 'certificate'}, inplace=True)


# Rename the column 'runtime' to 'duration_min' from data 4
data4.rename(columns={'runtime' : 'duration_min'}, inplace=True)


# Rename the column 'user rating size' to 'votes' from data 2
data2.rename(columns={'user rating size' : 'votes'}, inplace=True)


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())

print("\nData 4:")
print(data4.head())


# Rename the column 'runtime' to 'duration_min'
data3.rename(columns={'runtime': 'duration_min'}, inplace=True)


# Add a decimal place to the 'duration_min' column
data3['duration_min'] = data3['duration_min'].astype(float).round(1)


# Check unique values in the 'language' column
print(data3['language'].unique())


# Count the occurrences of each unique value in 'language'
print(data3['language'].value_counts())


# Get a summary of the dataframe 3
print(data3.info())


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())

print("\nData 4:")
print(data4.head())


# Check unique values in the 'type' column
print(data4['type'].unique())


# Change values in the 'type' column to have only the first letter capitalized
data4['type'] = data4['type'].str.capitalize()


# Display the first few rows of each dataset to understand their structure
print("Data 1:")
print(data1.head())

print("\nData 2:")
print(data2.head())

print("\nData 3:")
print(data3.head())

print("\nData 4:")
print(data4.head())





# List of datasets to merge
datasets = [data1, data2, data3, data4]


# Concatenate the datasets
merged_data = pd.concat(datasets, ignore_index=True)


# Fill missing numeric values with 0
numeric_columns = merged_data.select_dtypes(include=['number']).columns
merged_data[numeric_columns] = merged_data[numeric_columns].fillna(0)


# Fill missing non-numeric values with 'Unknown'
non_numeric_columns = merged_data.select_dtypes(exclude=['number']).columns
merged_data[non_numeric_columns] = merged_data[non_numeric_columns].fillna('Unknown')


# Display the merged dataset
print(merged_data.head())


# Save each cleaned dataset
data1.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_data1.csv', index=False)
data2.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_data2.csv', index=False)
data3.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_data3.csv', index=False)
data4.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_data4.csv', index=False)


# Save the merged dataset
merged_data.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\merged_data.csv', index=False)


# Store all datasets (including intermediate ones) in a single Excel file with different sheets
with pd.ExcelWriter(r'C:\Users\saman\Downloads\FP_BA_Netflix\all_datasets.xlsx') as writer:
    data1.to_excel(writer, sheet_name='Clean_Data1', index=False)
    data2.to_excel(writer, sheet_name='Clean_Data2', index=False)
    data3.to_excel(writer, sheet_name='Clean_Data3', index=False)
    data4.to_excel(writer, sheet_name='Clean_Data4', index=False)
    merged_data.to_excel(writer, sheet_name='Merged_Data', index=False)





# Show the first few rows of the dataset
print(merged_data.head())


# Get a summary of the dataset, including data types and non-null counts
print(merged_data.info())


# Show basic statistics for numeric columns
print(merged_data.describe())


# Check for missing values in the dataset
print(merged_data.isnull().sum())


# Find and remove duplicate rows
duplicates = merged_data[merged_data.duplicated()]


# Count duplicates
duplicate_count = merged_data.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_count}")


# Display the duplicated rows
duplicates = merged_data[merged_data.duplicated()]
print(duplicates)


# Display all rows that are duplicates (including the first occurrence)
duplicates_inclusive = merged_data[merged_data.duplicated(keep=False)]
print(duplicates_inclusive)


# Check duplicate rows based on specific columns (if you suspect certain columns)
duplicates_by_columns = merged_data[merged_data.duplicated(subset=['title'])]
print(duplicates_by_columns)


# Define a function to count the non-missing values
def count_non_missing(row):
    # Replace 'Unknown' and 0 with NaN for easier counting of valid values
    return row.replace(['Unknown', 0], [pd.NA, pd.NA]).notna().sum()


# Apply the function to each row and create a new column 'non_missing_count'
merged_data['non_missing_count'] = merged_data.apply(count_non_missing, axis=1)


# Sort the data first by 'non_missing_count' in descending order, then by 'votes' in descending order
merged_data = merged_data.sort_values(by=['non_missing_count', 'votes'], ascending=[False, False])


# Remove duplicates based on the 'title' and 'year' columns, keeping the first (best) row
merged_data_cleaned = merged_data.drop_duplicates(subset=['title', 'year'], keep='first')


# Drop the helper 'non_missing_count' column
merged_data_cleaned = merged_data_cleaned.drop(columns=['non_missing_count'])


# Display the cleaned dataset
print(merged_data_cleaned)


# Save the cleaned dataset to a CSV file
output_path = r'C:\Users\saman\Downloads\FP_BA_Netflix\cleaned_merged_data.csv'
merged_data_cleaned.to_csv(output_path, index=False)


# Use ExcelWriter to save all datasets in one Excel file with different sheets
with pd.ExcelWriter(r'C:\Users\saman\Downloads\FP_BA_Netflix\all_datasets.xlsx') as writer:
    data1.to_excel(writer, sheet_name='Clean_Data1', index=False)
    data2.to_excel(writer, sheet_name='Clean_Data2', index=False)
    data3.to_excel(writer, sheet_name='Clean_Data3', index=False)
    data4.to_excel(writer, sheet_name='Clean_Data4', index=False)
    merged_data.to_excel(writer, sheet_name='Merged_Data', index=False)
    merged_data_cleaned.to_excel(writer, sheet_name='Cleaned_Merged_Data', index=False)


# Check for missing values in the dataset
print(merged_data_cleaned.isnull().sum())


# Check unique values in the 'genre' column
print(merged_data_cleaned['genre'].unique())


# Capitalize the first letter of all string values in the dataset
for col in merged_data_cleaned.select_dtypes(include=['object']).columns:
    merged_data_cleaned[col] = merged_data_cleaned[col].str.title()


# Display the first few rows to verify the changes
print(merged_data_cleaned.head())


# Get the frequency distribution of the 'genre' column
genre_frequency = merged_data_cleaned['genre'].value_counts()

# Display the frequency distribution
print(genre_frequency)


# Calculate percentage for each genre
genre_percentages = (genre_frequency / genre_frequency.sum()) * 100

# Combine frequency and percentage into a single DataFrame
genre_distribution = pd.DataFrame({
    'Frequency': genre_frequency,
    'Percentage (%)': genre_percentages.round(2)
})


# Display the frequency distribution with percentage
print(genre_distribution)


# Convert values in 'production_countries' and 'certificate' columns to uppercase
merged_data_cleaned['production_countries'] = merged_data_cleaned['production_countries'].str.upper()
merged_data_cleaned['certificate'] = merged_data_cleaned['certificate'].str.upper()


# Display the first few rows to verify the changes
print(merged_data_cleaned[['production_countries', 'certificate']].head())


# Display the first few rows to verify the changes
print(merged_data_cleaned.head())


# Update the 'description' column to follow standard paragraph capitalization
merged_data_cleaned['description'] = merged_data_cleaned['description'].apply(
    lambda x: x.capitalize() if isinstance(x, str) else x
)


# Display the first few rows to verify the changes
print(merged_data_cleaned['description'].head())


# Display the first few rows to verify the changes
print(merged_data_cleaned.head())


# Display the info of the data
print(merged_data_cleaned.info())


# Save each cleaned dataset
data1.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_data1.csv', index=False)
data2.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_data2.csv', index=False)
data3.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_data3.csv', index=False)
data4.to_csv(r'C:\Users\saman\Downloads\FP_BA_Netflix\clean_data4.csv', index=False)


# Save the cleaned dataset to a CSV file
output_path = r'C:\Users\saman\Downloads\FP_BA_Netflix\cleaned_merged_data.csv'
merged_data_cleaned.to_csv(output_path, index=False)


# Use ExcelWriter to save all datasets in one Excel file with different sheets
with pd.ExcelWriter(r'C:\Users\saman\Downloads\FP_BA_Netflix\all_datasets.xlsx') as writer:
    data1.to_excel(writer, sheet_name='Clean_Data1', index=False)
    data2.to_excel(writer, sheet_name='Clean_Data2', index=False)
    data3.to_excel(writer, sheet_name='Clean_Data3', index=False)
    data4.to_excel(writer, sheet_name='Clean_Data4', index=False)
    merged_data.to_excel(writer, sheet_name='Merged_Data', index=False)
    merged_data_cleaned.to_excel(writer, sheet_name='Cleaned_Merged_Data', index=False)



